---
title: "Estatística no R"
author: "Maurício Bueno"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  word_document:
    toc: yes
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

Esta disciplina tem como objetivo aprender a realizar análises estatísticas mais empregadas em pesquisas em psicologia, no ambiente de programação R.
Inicialmente, vamos carregar (`install.packages()) e ativar (library()`) os pacotes que iremos utilizar na disciplina.

# Carregamento dos pacotes

```{r}
if(!require(tidyverse)) install.packages("tidyverse")
if(!require(psych)) install.packages("psych")
if(!require(knitr)) install.packages("knitr")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(expss)) install.packages("expss")
if(!require(janitor)) install.packages("janitor")
if(!require(pander)) install.packages('pander')
if(!require(arsenal)) install.packages('arsenal')
if(!require(moments)) install.packages('moments')
if(!require(readr)) install.packages('moments')
if(!require(readxl)) install.packages('readxl')
if(!require(descr)) install.packages("descr")
if(!require(rcompanion)) install.packages("rcompanion")
if(!require(rstatix)) install.packages("rstatix")
# Ativação dos pacotes ====

library(tidyverse)
library(psych)
library(knitr)
library(kableExtra)
library(expss)
library(janitor)
library(pander)
library(arsenal)
library(readxl)
library(readr)
library(rstatix)
library(corrplot)
library(GPArotation)
```

# Importar banco de dados

Inicialmente vamos trabalhar com o banco de dados `dataset_mapfre.csv`, clique [aqui](https://docs.google.com/spreadsheets/d/1G0lQKeU0atMk3Q4wO5zz4HdvReRxKS68vx_Q0WbB37A/edit?usp=sharing) para baixá-lo.
Esse banco de dados se refere ao artigo "[Sintomas de depressão e ansiedade em uma amostra representativa de universitários espanhóis, portugueses e brasileiros](https://drive.google.com/file/d/1qa0iCDm2DRvh3M6a2k7ENGahceDAlPIg/view?usp=sharing)".
Sugere-se a leitura desse artigo para a compreensão dos dados.

```{r}
Dataset <- read.csv("dataset_mapfre.csv",encoding="UTF-8",stringsAsFactors=TRUE)

# OBS: Para que este comando funcione, é necessário que o arquivo csv esteja no diretório de trabalho do R.

# Observar o banco de dados
glimpse(Dataset)
```

# Estatística descritiva

## Frequências

```{r}

# Função count()
Dataset %>% 
  filter(!is.na(sex)) %>% 
  group_by(sex) %>% 
  count(country) %>% 
  mutate(porc = n/sum(n)*100) %>% 
  pander()

# usando a função tabyl do janitor
# contar quantas pessoas de cada país.

Dataset %>%             # pegar o dataframe Dataset
  tabyl(country) %>%    # use a função tabyl para tabular as categorias de país
  pander()              # utilize o pander para fazer a tabela

Dataset %>%             # pegar o dataframe Dataset
  tabyl(country) %>%    # use a função tabyl para tabular as categorias de país
  adorn_totals() %>%    # acrescente uma linha com os totais
  pander()              # utilize o pander para fazer a tabela

# contar por gênero e país (retirando NAs)
Dataset %>% 
  filter(!is.na(sex)) %>% 
  tabyl(sex,country) %>% 
  adorn_totals() %>% 
  pander()

# contar por gênero e país, acrescentando funções
Dataset %>% 
  filter(!is.na(sex)) %>% 
  tabyl(country,sex) %>%           # função de tabulação do janitor
  adorn_totals(c("row","col")) %>% # adiciona o N
  adorn_percentages("row") %>%     # adiciona porcentagens
  adorn_pct_formatting(
    rounding = "half up",          # arredondar do cinco pra cima
    digits = 0) %>%                # número de casas decimais
  adorn_ns() %>%                   # mostra N e % juntas
  pander()                         # melhora a visualização dos dados.

# adição de um qui-quadrado para a distribuição
# para rodar essa análise tem que tirar os totais.

# Dataset %>% 
  # filter(!is.na(sex)) %>% 
  # tabyl(country,sex) %>%           # função de tabulação do janitor
  # adorn_totals(c("row","col")) %>% # adiciona o N
  # chisq.test()
  # pander()                         # melhora a visualização dos dados.
```

#### Exercício

Utilize o dataframe `Dataset` para realizar os exercícios abaixo:

1.  Crie uma tabela de frequências dos cursos (grado) dos participantes.

2.  Crie uma tabela de frequências dos cursos dos participantes, por sexo, com totais.

```{r message=FALSE, warning=FALSE, include=FALSE}
Dataset %>%
  tabyl(grado) %>%  
  adorn_totals() %>%
  pander()          

Dataset %>% 
  filter(!is.na(sex)) %>% 
  tabyl(grado,sex) %>%           
  adorn_totals(c("row","col")) %>% 
  adorn_percentages("row") %>%     
  adorn_pct_formatting(rounding = "half up", digits = 0) %>% 
  adorn_ns() %>%
  pander()      
```

## Medidas de tendência central e de dispersão

A descrição dos dados geralmente é realizada pelas seguintes estatísticas: Média, desvio padrão, mediana, amplitude, mínimo, máximo, assimetria e curtose.
Para calculá-las podemos usar funções específicas para cada uma ou funções que realizam um conjunto de análises descritivas.

### Funções específicas

```{r results="asis"}

## mean()
mean(Dataset$age, na.rm = TRUE)

## sd()
sd(Dataset$age, na.rm = TRUE)

## median()
median(Dataset$age, na.rm = TRUE)

## min()
min(Dataset$age, na.rm = TRUE)

## max()
max(Dataset$age, na.rm = TRUE)

## range()
range(Dataset$age, na.rm = TRUE)

## skewness()
moments::skewness(Dataset$age, na.rm = TRUE)

## kurtosis()
moments::kurtosis(Dataset$age, na.rm = TRUE)
```

#### Exercícios

Calcule as estatísticas descritivas: média, desvio padrão, mediana, valores mínimo e máximo, amplitude, assimetria e curtose das pontuações em depressão (bdi_sum) e ansiedade (bai_sum).
Crie uma tabela (dataframe) com esses valores.

```{r}
# média
mean(Dataset$bdi_sum, na.rm = TRUE)
mean(Dataset$bai_sum, na.rm = TRUE)

# desvio padrão
sd(Dataset$bdi_sum, na.rm = TRUE)
sd(Dataset$bai_sum, na.rm = TRUE)

# mínimo
min(Dataset$bdi_sum, na.rm = TRUE)
min(Dataset$bai_sum, na.rm = TRUE)

# máximo
max(Dataset$bdi_sum, na.rm = TRUE)
max(Dataset$bai_sum, na.rm = TRUE)

# amplitude
range(Dataset$bdi_sum, na.rm = TRUE)
range(Dataset$bai_sum, na.rm = TRUE)

# assimetria
moments::skewness(Dataset$bdi_sum, na.rm = TRUE) # curva assimétrica positiva (rabo para a direita)
moments::skewness(Dataset$bai_sum, na.rm = TRUE) # curva assimétrica positiva (rabo para a direita)

# curtose
moments::kurtosis(Dataset$bdi_sum, na.rm = TRUE) # curva leptocúrtica (pontuda)
moments::kurtosis(Dataset$bai_sum, na.rm = TRUE) # curva leptocúrtica (pontuda)

# Tabela

data.frame(Estatísticas =       # data.frame() é a função para criar uma tabela ou planilha
             c("Média",         # eu quero três colunas: estatísticas, depressão e ansiedade
               "Desvio Padrão", # então, precisamos definir o que vai aparecer em cada coluna   
               "Mínimo",
               "Máximo",
               "Assimetria",
               "Curtose"),
           Depressão = 
             c(mean(Dataset$bdi_sum, na.rm = TRUE),
               sd(Dataset$bdi_sum, na.rm = TRUE),
               min(Dataset$bdi_sum, na.rm = TRUE),
               max(Dataset$bdi_sum, na.rm = TRUE),
               moments::skewness(Dataset$bdi_sum, na.rm = TRUE),
               moments::kurtosis(Dataset$bdi_sum, na.rm = TRUE)),
           Ansiedade = 
             c(mean(Dataset$bai_sum, na.rm = TRUE),
               sd(Dataset$bai_sum, na.rm = TRUE),
               min(Dataset$bai_sum, na.rm = TRUE),
               max(Dataset$bai_sum, na.rm = TRUE),
               moments::skewness(Dataset$bai_sum, na.rm = TRUE),
               moments::kurtosis(Dataset$bai_sum, na.rm = TRUE))) %>% 
  pander()
```

# Distribuição normal

A distribuição normal (ou distribuição de Gauss) é uma distribuição de probabilidades, isto é ela indica a probabilidade de um certo valor ser obtido.
Uma **distribuição normal padrão** apresenta médial igual a zero e desvio padrão igual a 1.
A média fica no centro da distribuição e coincide com a mediana, separando a amostra em duas metades.
Quando a curtose (achatamento) e assimetria (distribuição lateral) são diferentes de zero, a distribuição começa a se afastar da normal.

```{r}
distalea <- runif(n=100000, min=1, max=5)
hist(distalea)
mean(distalea)
sd(distalea)

# Retirar 40 amostras da população distalea, calcular as médias e montar uma lista
distalea_mean_list <- c(mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)),
                        mean(sample(distalea, 100, replace = FALSE)))


hist(distalea_mean_list)

# Criar / Extrair amostra com valores inteiros

sample(
  x = 1:5,
  size = 1000,
  replace = TRUE
) %>% hist()

# usando a função rnorm() para gerar dados aleatórios 
# rnorm(n, mean = 10, sd = 2)

# gerar 
distnorm <- rnorm(100000, mean = 10, sd = 2)
mean(distnorm)
sd(distnorm)
hist(distnorm, breaks = 50)
range(distnorm)
skew(distnorm)
kurtosi(distnorm)

sample(distnorm,          # nome do arquivo
       100,               # número de observações que se deseja extrair
       replace = FALSE)   # False indica que a amostra será extraída sem reposição.


s1 <- sample(distnorm, 100, replace = FALSE)
s2 <- sample(distnorm, 100, replace = FALSE)
s3 <- sample(distnorm, 100, replace = FALSE)
s4 <- sample(distnorm, 100, replace = FALSE)
s5 <- sample(distnorm, 100, replace = FALSE)
s6 <- sample(distnorm, 100, replace = FALSE)
s7 <- sample(distnorm, 100, replace = FALSE)
s8 <- sample(distnorm, 100, replace = FALSE)
s9 <- sample(distnorm, 100, replace = FALSE)
s10 <- sample(distnorm, 100, replace = FALSE)
s11 <- sample(distnorm, 100, replace = FALSE)
s12 <- sample(distnorm, 100, replace = FALSE)
s13 <- sample(distnorm, 100, replace = FALSE)
s14 <- sample(distnorm, 100, replace = FALSE)
s15 <- sample(distnorm, 100, replace = FALSE)
s16 <- sample(distnorm, 100, replace = FALSE)
s17 <- sample(distnorm, 100, replace = FALSE)
s18 <- sample(distnorm, 100, replace = FALSE)
s19 <- sample(distnorm, 100, replace = FALSE)
s20 <- sample(distnorm, 100, replace = FALSE)

mean(s1 )
mean(s2 )
mean(s3 )
mean(s4 )
mean(s5 )
mean(s6 )
mean(s7 )
mean(s8 )
mean(s9 )
mean(s10)
mean(s11)
mean(s12)
mean(s13)
mean(s14)
mean(s15)
mean(s16)
mean(s17)
mean(s18)
mean(s19)
mean(s20)

médias <- c(mean(s1),mean(s2),mean(s3),mean(s4),mean(s5),mean(s6),mean(s7),mean(s8),mean(s9), mean(s10),
            mean(s11),mean(s12),mean(s13),mean(s14),mean(s15),mean(s16),mean(s17),mean(s18),mean(s19), mean(s20))

mean(médias)
sd(médias)
hist(médias,breaks = 5)

mean(médias) - sd(médias) # um desvio abaixo da média
mean(médias) - 2*sd(médias) # dois desvios abaixo da média

mean(médias) + sd(médias) # um desvio abaixo da média
mean(médias) + 2*sd(médias) # dois desvios abaixo da média

#mean(c(mean(s1),mean(s2),mean(s3),mean(s4),mean(s5),mean(s6),mean(s7),mean(s8),mean(s9), mean(s10)))
#sd(c(mean(s1),mean(s2),mean(s3),mean(s4),mean(s5),mean(s6),mean(s7),mean(s8),mean(s9), mean(s10)))
```

# Funções que sumariam dados

```{r}
# describe() {psych}
describe(Dataset$age, na.rm = TRUE) %>% pander()
describe(Dataset$age, na.rm = TRUE) %>% kable(format = "markdown")

# summary() {base}
Dataset %>% select(age) %>% summary()

# tableby() {arsenal}
tableby(country ~ bdi_sum + bai_sum, # calcule as descritivas de bdi e bai por country 
        test = FALSE,                # se FALSE,  não faz teste de significância 
        data = Dataset) %>%          # especificar base de dados
  summary(text=TRUE)                 # tem que colocar text=TRUE senão aparecem uns códigos estranhos porque o R entende o espaço como código.                                            Assim a tabela fica bacaninha.

## mesma função mas para a variável sexo.
tableby(sex ~ bdi_sum + bai_sum,
        test = FALSE,
        data = Dataset) %>% 
  summary(text=TRUE)

## Pode descrever apenas uma variável em função de sexo
tableby(sex ~ age, data = Dataset) %>% summary(text = TRUE)

## Pode descrever somenta a variável contínua sem ser em função de alguma outra variável.
tableby( ~ age + bdi_sum + bai_sum, data = Dataset) %>% summary(text = TRUE)

## Também pode dar as médias em função do sexo, estratificado por país
tableby(sex ~ age + bdi_sum + bai_sum, data = Dataset, strata = country) %>% summary(text = TRUE)

## ou o contrário
tableby(country ~ age + bdi_sum + bai_sum, data = Dataset, strata = sex) %>% summary(text = TRUE)

## Interação entre variáveis nominais

tableby(interaction(sex, country) ~ bdi_sum + bai_sum,
        test = FALSE,
        data = Dataset) %>% 
  summary(text = TRUE)
```

# Representações gráficas

### Gráficos de barras

```{r}
ggplot(Dataset, aes(x = country)) + 
  geom_bar() + 
  labs(x = "país",
       title = "Número de participantes nos países investigados")

# alterando o eixo y para ser a % ao invés do N
ggplot(Dataset, aes(x = country, y = ..prop.., group = 1)) + 
  geom_bar(stat = "count",fill = "dodgerblue4") +
  geom_text(aes(label = scales::percent(round(..prop..,2)),
                y = ..prop..),
            stat = "count", 
            color = "white",
            size = 5, 
            position = position_stack(vjust = 0.5)) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(x = "país",
       y = "porcentagem",
       title = "Número de participantes nos países investigados")

# Alterando argumentos
ggplot(Dataset, 
       aes(x=country, y=bai_sum)) +
  geom_bar(stat = "summary", 
           fun = mean, 
           fill = "dodgerblue", 
           color = "black",
           na.rm = TRUE) + 
  labs(title = "Média de Ansiedade por país",
       x = "País", y = "Média de Ansiedade (BAI)")

# adicionando outros elementos ao gráfico (barras de erro)
ggplot(Dataset, 
       aes(x=country, y=bai_sum)) +
  geom_bar(stat = "summary", 
           fun = mean, 
           fill = "violet", 
           color = "black",
           na.rm = TRUE) + 
  stat_summary(geom = "errorbar", fun.data = mean_se,width = 0.5) +
  labs(title = "Média de Ansiedade por país",
       x = "País", y = "Média de Ansiedade (BAI)")

# se modificar a posição do argumento fill, para dentro de aes e com o critério country... o gráfico fica colorido por país.
ggplot(Dataset, 
       aes(x=country, y=bai_sum,fill = country)) +
  geom_bar(stat = "summary", 
           fun = mean, 
           color = "black",
           na.rm = TRUE) + 
  stat_summary(geom = "errorbar", fun.data = mean_se,width = 0.5) +
  labs(title = "Média de Ansiedade por país",
       x = "País", y = "Média de Ansiedade (BAI)")
```

### Gráfico de setor (polar, pizza ou torta)

```{r}
Dataset %>% 
  count(country) %>% 
  mutate(pct = n/sum(n)) %>% 
  ggplot(.,aes(x = "", y = pct, fill = country)) + 
  geom_col(color = "black") + 
  geom_text(aes(label = scales::percent(round(pct,3))),
            position = position_stack(vjust = 0.5)) + 
  coord_polar(theta = "y") + 
  labs(title = "Proporção de participantes em cada país")
```

### Gráficos para descrição de distribuição ou variabilidade

```{r}
# Histograma
ggplot(data = Dataset, aes(x = age)) + 
  geom_histogram(bins = 30,color = "black", fill = "#61988E") + 
  labs(y = "Frequência",
    title = "Distribuição da idade dos participantes")

# Densidade
ggplot(data = Dataset, aes(age)) + 
  geom_density(fill = "#56b4e9") + 
  labs("Distribuição da idade dos participantes")

# Boxplot (diagrama de caixa e bigode)
## Uma variável continua
ggplot(data = Dataset, aes(y=age,x="")) +
  geom_boxplot(fill = "#56b4e9") + 
  labs(title = "Distribuição da idade dos participantes")

## uma variável continua e uma categórica (discreta)
## no caso, bai_sum por país.
ggplot(Dataset,aes(x=country, y=bai_sum)) +
  geom_boxplot(color = "black", fill = "#74c69d") + 
  labs(title = "Distribuição das Pontuações no BAI, por país")


# Combinação de gráficos usando o grid.arrange do pacote gridExtra

gridExtra::grid.arrange(
  
# Gráfico 1
ggplot(data = Dataset, aes(age)) + 
  geom_histogram(aes(y=..density..), alpha = 0.5, position = "identity") +
  geom_density(alpha = 0.8, fill = "#56b4e9") + 
  labs("Distribuição da idade dos participantes"),
# Gráfico 2
ggplot(data = Dataset, aes(y=age,x="")) +
  geom_boxplot(fill = "#56b4e9") + 
  labs(x = "") +
  coord_flip(),
top = "Distribuição da idade dos participantes"
)
```

### Gráficos de pontos

```{r}
ggplot(Dataset, aes(x=age, y=bai_sum)) +
  geom_point(color = "#9e2a2b") +
  labs(title = "Idade x Ansiedade",
       x = "Idade", y = "Ansiedade (BAI)")

ggplot(Dataset, aes(x=age, y=bai_sum)) +
  geom_jitter(color = "#9e2a2b") +
  labs(title = "Idade x Ansiedade",
       x = "Idade", y = "Ansiedade (BAI)")

# com a reta de regressão
ggplot(Dataset, aes(x=age, y=bai_sum)) +
  geom_jitter(color = "#9e2a2b") +
  geom_smooth(method = "lm") +
  labs(title = "Idade x Ansiedade",
       x = "Idade", y = "Ansiedade (BAI)")

# ansiedade x depressão
  
ggplot(Dataset, aes(x=bai_sum, y=bdi_sum)) +
  geom_jitter(color = "#9e2a2b") +
  geom_smooth(method = "lm") +
  labs(title = "Depressão x Ansiedade",
       x = "Depressão (BDI)", y = "Ansiedade (BAI)")

# colocar os dois gráficos juntos na mesma imagem

gridExtra::grid.arrange(
  
  #Gráfico 1
  
  ggplot(Dataset, aes(x=age, y=bai_sum)) +
  geom_point(color = "#9e2a2b") +
  labs(title = "Idade x Ansiedade",
       x = "Idade", y = "Ansiedade (BAI)"),

  # Gráfico 2
ggplot(Dataset, aes(x=age, y=bai_sum)) +
  geom_jitter(color = "#9e2a2b") +
  labs(title = "Idade x Ansiedade",
       x = "Idade", y = "Ansiedade (BAI)"),
nrow = 1    # sem esse comando, os gráficos ficam 2 linhas
)

Dataset %>% 
filter(!is.na(sex)) %>% 
  ggplot(.,aes(x = age, y = bai_sum, color = sex)) +
  geom_jitter() +
  geom_smooth(method = "lm")

Dataset %>% 
  filter(!is.na(sex), !is.na(curso_ou_ano)) %>% 
  ggplot(., aes(x = age, y=bai_sum,
                fill = factor(curso_ou_ano),
                color = sex,
                shape = country)) + 
  geom_jitter() +
  geom_smooth(method = "lm")
         
```

# Análises de associação entre variáveis

Muitas vezes, o pesquisador está interessado no estudo ou análise de relações entre variáveis.
Nesses casos, ele recorrerá a estatísticas como o teste de qui-quadrado (caso tenha variáveis do tipo nominal ou categórica), coeficiente de correlação de Pearson ou regressão linear quando as variáveis forem contínuas.

## Testes de Qui-Quadrado

O teste de qui-quadrado é uma medida de associação entre variáveis categóricas, das quais só temos a frequência de ocorrência.
Existem três tipos de testes de qui-quadrado:

-   de aderência: quando se deseja verificar as distribuições de probabilidades de cada categoria de uma variável em relação a um valor teórico esperado.

-   de homogeneirdade: quando se deseja verificar se as distribuições das categorias são as mesmas para diferentes subpopulações de interesse.

-   de independência: verficiar se duas variáveis categóricas são independentes

Neste caso, vamos fazer uma análise de qui-quadrado de independência, a mais comumumente utilizada, para verificar se há uma associação entre país e sexo dos participantes do banco de dados Dataset.
O primeiro passo para isso é montar uma tabela de contingência.

## Tabela de Contingência

Nesse caso, vamos usar a função da base `table()` para gerar uma tabela 2 x 3, duas linhas e três colunas, em que as linhas representarão os sexos masculino e feminino, e as colunas representarão os países: Brasil, Portugal e Espanha.

```{r}
tabcont_sex_country <- table(Dataset$sex,Dataset$country)

```

O próximo passo é realizar a análise de qui-quadrado em si.

## Análise de qui-quadrado.

```{r}
# É bom salvá-la em um objeto porque tem mais informações do que as que são mostradas na análise.
options(scipen = 999) # função para tirar a notação científica de potência.
chi_sex_country <- chisq.test(tabcont_sex_country)

# Pressuposto: frequências esperadas > 5
###chi_sex_country$expected
###round(chi_sex_country$expected,0)
# Análise dos resíduos
## Resíduo padronizado ou resíduo de Pearson
###chi_sex_country$residuals

## Resíduo padronizado ajustado (mais usado) - 
### Estão padronizados em z. 
### Portanto, valores > 1.96 ou < -1.96 são considerados valores significativos para p/ alfa de 5%
### ou seja, cujos resíduos encontrados são maiores do que os esperados.
###chi_sex_country$stdres

# Alguns autores recomendam um ajuste na análise do resíduo em função do tamanho da tabela de contingência
# Assim, novo_sig = 0,05/(n_linhas * n_colunas)
# a nova formula para calcular o índice de significância é:
new_alfa <- 0.05/(nrow(tabcont_sex_country)*ncol(tabcont_sex_country))

# calcular os pontos de corte em z para o novo valor de alfa (new_alfa)
###qnorm(0.05/2)       # valor de z correspondente a um alfa de 5%, que dá o 1,96
###qnorm(new_alfa/2)   # z relativo ao new_alfa: > 2,64 ou < -2,64, para alfa=0,83%
                    # Usar esse valor para avaliar os resíduso padr. ajustados
                    # com o novo valor de z, os resultados continuam significativos.
# uma opção seria calcular o p para todos os resíduos
###options(scipen = 0)
###2*(1-pnorm(abs(chi_sex_country$stdres)))

# Tamanho do efeito
# phi é usado para tabelas 2 x 2
# V de Cramer é usado para tabelas maiores
cramer_v(tabcont_sex_country)

# A interpretação do V de Cramer depende dos graus de liberdade do teste
# gl = (linhas -1) * (colunas-1)
# Messe caso gl=2 e o V de Cramer corresponde a um tamanho de efeito pequeno (Cohen, 1988).
# O V de Cramer varia de 0 a 1. Valores baixos e altos correspondem a efeitos pequenos e grandes, respectivamente.

# para calcular o phi (tamanho de efeito para tabelas 2 x 2)
# phi(tabela_de_contingência)
```

Nesse caso, o valor de p foi igual a 3.003e-11 (lê-se 3.003 x 10^-11^).
A interpretação do V de Cramer depende dos graus de liberdade empregada na análise.
Como essa análise foi realizada sobre uma distribuição 2 x 3, o número de graus de liberdade é igual a 2 (gl = (linhas -1) \* (colunas-1)).

```{r echo=FALSE}
data.frame("Graus de Liberdade" = c(1,2,3),
           Pequeno = c(0.1,0.07,0.06),
           Médio = c(0.3,0.21,0.17),
           Grande = c(0.5,0.35,0.29)) %>% pander()
```

Nesse caso, o valor de p foi menor que 0,05, indicando que há associação entre o sexo do participantes e o diagnóstico de TDAH.
Essa associação diz que ser do sexo masculino aumenta as chances de receber uma diagnóstico de TDAH.
No entanto, o tamanho do efeito foi pequeno.

## Representação gráfica

```{r}
corrplot::corrplot(chi_sex_country$stdres,  # função para representação em cores
                   is.corr = FALSE,         # não se trata de correlações
                   method = "color",        # método para pintar o quadrado todo.
                   tl.col = "black",        # textos na cor preta
                   tl.srt = 0)              # angulação das colunas, 90 é vertical.

ggplot(Dataset, aes(x = country, fill = sex)) +
  geom_bar(position = "fill") +
  coord_flip() +
  labs(title = "Proporção Sexo x País",
       x = "País", y = "Proporção", fill = "Sex")
```

# Exercícios

1.  Calcular o qui-quadrado para verificar as associações entre sexo e ansiedade (bai_class)

2.  Calcular o qui-quadrado para verificar as associações entre sexo e depressão (bai_class)

Dica: para que as categorias de ansiedade e depressão sejam mostradas em ordem crescente (mínima, leve, moderada, grave), é necessário informar o R disso por meio do argumento `levels`da função `factor`.

```{r}

# colocando as variáveis bdi_class e bai_class em ordem, usando o levels
summary(Dataset$country)
summary(Dataset$sex)
summary(Dataset$bdi_class)
summary(Dataset$bai_class)

# padronizar a ordem dos níveis: minima, leve, moderada, grave.
Dataset$bdi_class <- factor(Dataset$bdi_class,
                            levels = c("minima",
                                       "leve",
                                       "moderada",
                                       "grave"))

Dataset$bai_class <- factor(Dataset$bai_class,
                            levels = c("minima",
                                       "leve",
                                       "moderada",
                                       "grave"))
```

## Coeficiente de correlação de Pearson

Quando o interesse do pesquisador é investigar o grau de associação entre variáveis, ele pode empregar o Coeficiente de Correlação de Pearson.
Os resultados dessa análise podem variar de -1 a 1.
Resultados positivos revelam associação diretamente proporcional entre as variáveis, resultados negativos revelam associação inversamente proporcional entre as variáveis e resultados próximos de zero indicam que não há associação entre as variáveis.

Usando o dataframe `Dataset`, vamos calcular o coeficiente de correlação entre depressão e ansiedade:

```{r}

# verificação de normalidade
options(scipen = 999)
shapiro.test(Dataset$bai_sum)
shapiro.test(Dataset$bdi_sum)

## representação gráfica da normalidade (histograma)
hist(Dataset$bdi_sum)
hist(Dataset$bai_sum)


# presença de outliers
boxplot(Dataset$bdi_sum)
boxplot(Dataset$bai_sum)

# Relação linear entre as variáveis
plot(Dataset$bdi_sum,Dataset$bai_sum)

# Análise de resíduos (homocedasticidade)
lm_ans_dep <- lm(bdi_sum ~ bai_sum, Dataset)

par(mfrow = c(1,2)) # os números indicam uma linha, duas colunas
plot(lm_ans_dep, which = c(1,3))
# os gráficos mostram os resíduos pelos valores previstos.
# o 1º mostra os resíduos brutos e o 2º ow resíduos padronizados
# Em ambos os casos a linha deve estar paralela a x, o que indica relação linear entre as variáveis.
# Em ambos os casos os pontos devem estar homogeneamente distribuídos ao longo da reta, distribuição de pontos igualitária ao longo da curva (homocedasticidade). A variação dos resíduos tem que ser homogênea ao longo da curva.
# Verificação de outliers quando há resíduos acima de 3 ou abaixo de -3
par(mfrow = c(1,1)) # voltar para um gráfico por vez

# correlação de Pearson (r)
options(scipen = 0)
cor.test(Dataset$bdi_sum,Dataset$bai_sum, method = "pearson") %>% pander()

# correlação de Spearman (rho)
cor.test(Dataset$bdi_sum,Dataset$bai_sum, method = "spearman") %>% pander()

# correlação de Kendal (Tau)
cor.test(Dataset$bdi_sum,Dataset$bai_sum, method = "kendall") %>% pander()

```

## Matriz de correlações

A maioria das vezes, os pesquisadores querem observar as correlações entre mais de duas variáveis.
Nesses casos, é necessário criar uma matriz de correlações.
Para fazer isso, vamos usar o banco de dados `big_five` e a função `corrplot` do pacote `corrplot`.

```{r}
# saveRDS(big_five, file = "big_five.rds")
big_five <- readRDS("big_five.rds")

glimpse(big_five)
names(big_five)

# obter matriz de correlações
matriz <- cor(big_five[,59:63], method = "pearson")

matrix <- corr.test(big_five[,59:63], method = "pearson") # usando o psych

matrix$r
matrix$p
matrix$stars %>% pander()

# matriz de correlações com linhas e colunas diferentes
names(big_five)
matrix1 <- corr.test(big_five[,59:63], big_five[,3], method = "pearson")
matrix1$r
matrix1$p
matrix1$stars %>% pander() # esse tipo de matriz não dá pra plotar.
```

## Representação gráfica das correlações

```{r}
ggplot(Dataset, aes(x=bdi_sum,y=bai_sum)) +
  geom_jitter() +
  geom_smooth(method = "lm") +
  labs(title ="Correlação entre Depressão e Ansiedade", 
       x = "Depressão", 
       y = "Ansiedade") +
  theme_classic()

# matriz de correlações usando o corrplot
corrplot(matriz, 
         method = "shade",
         type = "lower",
         order = "hclust",
         addCoef.col = TRUE)

# opções de método: "circle", "square", "ellipse", "number", "shade", "color", "pie
# opções de tipo: "lower", "upper", "full"
# opção de ordem: "hclust"- organiza as correlações hierarquicamente
#                 "original" - ordem das variáveis no banco
#                 "AOE" - angular order of eigenvectors
#                 "alphabet" - ordem alfabética
```

## Análise de Regressão

### Carregamento dos pacotes e importação do banco de dados

```{r}

if(!require(car)) install.packages("car")
if(!require(lmtest)) install.packages("lmtest")
if(!require(ggpubr)) install.packages("ggpubr")
if(!require(QuantPsyc)) install.packages("QuantPsyc")
if(!require(scatterplot3d)) install.packages("scatterplot3d")

library(car)
library(lmtest)
library(ggpubr)
library(QuantPsyc)
library(scatterplot3d)

```

O modelo linear

```{r}

# rodando a regressão

glimpse(big_five)
big_five_BR <- big_five %>% filter(país == "BR")
mod <- lm(idade ~ extr+neur+amab+cons+aber, # big-five predizendo idade
          data=big_five_BR,                 # onde estão os dados
          na.action = na.omit)              # ignorar NAs

## Análise dos pressupostos
par(mfrow = c(2,2))
plot(mod)

### O primeiro gráfico permite avaliar a linearidade. A linha vermelha tem que estar aproximadamente horizontal para o modelo ser linear.
### O segundo gráfico permite verificar se os resíduos apresentam distribuição normal (Q-Q plot). No eixo y os resíduos encontrados e no eixo x os resíduos que seriam esperados caso a distribuição fosse de fato normal. O esperado é que os pontos caiam sobre a linha pontilhada
### O terceiro gráfico avalia homocedasticidade. Para que haja homocedasticidade os pontos têm que estar distribuídos aleatoriamente pelo retângulo do gráfico e não formando um triângulo. Ou seja, os resíduos têm que ser aleatórios e não correlacionados entre si.
### No quarto gráfico é possível verificar a existência de outliers e pontos influentes. Caso existam outliers haverá pontos para fora da linha pontilhada vermelha. Os resíduos padronizados devem estar entre -3 e +3 (eixo y).

par(mfrow = c(1,1))   # voltar para 1 gráfico por página.

## normalidade dos resíduos
options(scipen = 999)
shapiro.test(mod$residuals)

## Outliers nos resíduos
summary(rstandard(mod)) # os resultados devem estar entre -3 e +3

## Teste de Durbin-Watson
durbinWatsonTest(mod)   
### não há autocorrelação entre os resíduos quando o valor da estatística está próximo a 2 (entre 1,5 e 2,5, ou entre 1 e 3)

## Homocedasticidade (Breusch-Pagan) ou homogeneidade das variâncias
bptest(mod)   # o valor de p > 0,05 indica que há homocedasticidade

## Ausência de multicolinearidade
names(big_five_BR)
pairs.panels(big_five_BR[,59:63]) # há multicolinearidade quando r > 0.9

vif(mod)   
### há problema de multicolinearidade quando vif > 10



## Interpretação do modelo
summary(mod)

# estatística F: comparação do modelo real com o modelo nulo (sem variáveis independentes). Então, só faz sentido usar o modelo real se ele for melhor que o modelo nulo. Para isso, o valor de p < 0,05.
# R-squared: porcentagem da variância que é explicada pelo modelo. O R-squared corrige pelo número de variáveis, o que permite comparar modelos com diferentes números de variáveis independentes. 

mod2 <- lm(idade ~ extr + cons, big_five_BR)

summary(mod2)

### Se olhar o R-squared do mod e mod2, pode-se ter uma ideia de qual modelo é melhor.

## Obtenção dos coeficientes padronizados pelo pacote (QuantPsyc)
lm.beta(mod)
lm.beta(mod2)


## Obtenção do IC 95% para os coeficientes
confint(mod)
### para o valor de p ser significativo, o intervalo de confiança não pode incluir o zero. às vezes isso não funciona por causa do métdo de análise das distribuições.
confint(mod2)

# Comparação de modelos
## AIC e BIC - Comparação entre quaisquer modelos

AIC(mod, mod2)   
### quanto menor, melhor
### representam a variância não explicada pelo modelo
### para um modelo ser melhor que o outro, a diferença deve ser pelo menos 10.

BIC(mod,mod2) # Modelo Bayesiano, que funciona da mesma forma.

# Comparação de modelos aninhados (o modelo 2 é derivado do modelo 1)
anova(mod,mod2)
### o melhor modelo será o valor de RSS menor (residual sum of squares)


# Representação gráfica
graph <- scatterplot3d(big_five_BR$idade ~ big_five_BR$extr + big_five_BR$cons,
                       pch = 16, angle = 45, color = "steelblue", box = FALSE,
                       xlab = "Extr", ylab = "Consc", zlab = "Idade")

graph$plane3d(mod2, col = "black", draw_polygon = TRUE)
```

